# Resumo do Projeto
Desenvolvimento de um MVP de plataforma de Data & Analytics voltada para o setor de com√©rcio de bebidas, com foco em transformar dados em insights acion√°veis. O projeto envolveu a implementa√ß√£o de pipelines de engenharia de dados utilizando PySpark, Python e SQL, aplica√ß√£o de modelagem dimensional (Star Schema) e defini√ß√£o de uma arquitetura em Cloud baseada em Data Lakehouse.
A solu√ß√£o foi projetada para responder aos principais KPIs de neg√≥cio, assegurando escalabilidade, desempenho e governan√ßa dos dados.

# Arquitetura Medalh√£o
<img src="https://media.licdn.com/dms/image/v2/D4D12AQHGTqz_KlZAWg/article-inline_image-shrink_1000_1488/B4DZaHqoAXHwAQ-/0/1746032821856?e=1762992000&v=beta&t=6QxtaUUJPY5NbY5UeWM2KsV8PDPAnJFt-HA9pNU2ls8" alt="Preview do projeto" width="900"/>


A arquitetura Medallion estrutura o ciclo de dados em tr√™s n√≠veis ‚Äî Raw/Bronze, Trusted/Silver e Refined/Gold ‚Äî promovendo uma jornada de evolu√ß√£o que assegura confiabilidade, padroniza√ß√£o e valor anal√≠tico crescente.

### üü§ Raw/Bronze ‚Äî Dados de Origem

- **Prop√≥sito:** Preservar as informa√ß√µes exatamente como foram coletadas, sem qualquer tratamento pr√©vio.
- **Fontes/A√ß√µes:** Registros de vendas no ponto de venda, logs de transporte e dados de sensores de estoque. Arquivos CSV contendo transa√ß√µes brutas de vendas di√°rias
- **Usu√°rios:** Engenheiros de dados e respons√°veis por ingest√£o.

### ‚ö™ Trusted/Silver ‚Äî Dados Tratados e Harmonizados

- **Prop√≥sito:** Padronizar e consolidar os dados da camada Bronze
- **Fontes/A√ß√µes:** Limpeza de inconsist√™ncias, remo√ß√£o de duplicidades, enriquecimento com cadastros de produtos e clientes. Dados oriundos de tabelas da camada Raw/Bronze
- **Usu√°rios:** Engenheiros, analistas e cientistas de dados que precisam de informa√ß√µes consistentes.

### üü° Refined/Gold ‚Äî Dados Otimizados para An√°lise

- **Prop√≥sito:** Disponibilizar dados refinados e de alto valor para uso estrat√©gico e operacional.
- **Fontes/A√ß√µes:** Cria√ß√£o de m√©tricas de neg√≥cio e indicadores como receita, margem, ticket m√©dio e diversos outros KPIs que possam gerar valor ao neg√≥cio. Dados oriundos de tabelas da camada Trud
/Silver
- **Usu√°rios:** Analistas de Dados, Stakeholders e Key Users.


### üîÑ Fluxo de Dados

1. Os dados oriundos das origens s√£o ingeridos na camada **Raw**.  
2. Limpeza, Transforma√ß√µes, Deduplica√ß√µes, transforma√ß√µes e valida√ß√µes na camada **Trusted**.  
3. Dados otimizados para an√°lise, agregados e prontos para utiliza√ß√£o de Stakeholders e ferramentas de BI na camada **Refined**.  

### üõ† Tecnologias Utilizadas

- **Databricks**
- **Unity Catalog**
- **PySpark**
- **Python**
- **SQL**
- **DataLake / DatalakeHouse**


# Pipeline
# Pipeline: job_lakehouse_felipe
O pipeline foi projetado para coletar, processar e organizar dados de forma automatizada em uma arquitetura Lakehouse, evoluindo de dados brutos at√© informa√ß√µes anal√≠ticas nas camadas Raw, Trusted e Refined.
O c√≥digo Python do pipeline est√° dispon√≠vel aqui nesse reposit√≥rio dentro do diret√≥rio 'controller'.

O pipeline est√° ativo e ser√° executado automaticamente com a chegada de novos arquivos (file_arrival) dentro do Volume `/Volumes/lakehouse/raw/files/`.


## Vis√£o Geral do Projeto
Este projeto apresenta a constru√ß√£o de um Delta Lakehouse com arquitetura dimensional voltado para an√°lises de performance de vendas
A modelagem segue o padr√£o de tabela fato e dimens√µes, composta por uma tabela de fatos (fact_order_item) e quatro tabelas dimensionais (dim_product_catalog, dim_region, dim_calendar e dim_trade_channel,).
Essa estrutura possibilita examinar resultados comerciais sob diferentes perspectivas ‚Äî tempo, canal, localiza√ß√£o e produto ‚Äî permitindo identificar tend√™ncias, sazonalidades e padr√µes de consumo.

## Estrutura do Delta Lakehouse

### Tabelas Dimens√£o

#### `dim_product_catalog`
- **skproduct_catalog**: Chave prim√°ria
- **product_catalog_id**: Identificador do produto
- **ce_brand_flvr, brand_nm**: Marca e sabor
- **pkg_cat, pkg_cat_desc, tsr_pckg_nm**: Categoria e embalagem
- **start_date, end_date**: Validade do registro
- **current**: Indica se √© registro atual
- **insertdate**: Timestamp de inser√ß√£o

####  `dim_region`
- **skregion**: Chave prim√°ria
- **place_id**: Identificador do local
- **Btlr_Org_LVL_C_Desc**: Descri√ß√£o da organiza√ß√£o/regi√£o
- **start_date, end_date**: Validade do registro
- **current**: Indica se √© registro atual
- **insertdate**: Timestamp de inser√ß√£o

####  `dim_calendar`
- **skcalendar**: Chave prim√°ria (BIGINT, auto incremento)
- **date**: Data
- **day, month, year**: Componentes da data
- **insertdate**: Timestamp de inser√ß√£o

####  `dim_trade_channel`
- **sktrade_channel**: Chave prim√°ria
- **trade_channel_id**: Identificador do canal
- **trade_chnl_desc, trade_group_desc, trade_type_desc**: Descri√ß√µes do canal
- **start_date, end_date**: Validade do registro
- **current**: Indica se √© registro atual
- **insertdate**: Timestamp de inser√ß√£o


---

### Tabela Fato

#### `fact_order_item`
- **skcalendar**: FK para `dim_calendar`
- **order_item_id**: Identificador da venda
- **skproduct_catalog**: FK para `dim_product_catalog`
- **skregion**: FK para `dim_region`
- **sktrade_channel**: FK para `dim_trade_channel`
- **volume**: Quantidade vendida
- **insertdate**: Timestamp de inser√ß√£o

---


